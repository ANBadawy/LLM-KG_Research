{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "GOLBAL_pages_number = 30\n",
    "\n",
    "# ====================================== Scrapping a book that is text \n",
    "def extraction_text_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        # Initialize an empty string to store the extracted text\n",
    "        extracted_text = ''\n",
    "        number_of_pages = GOLBAL_pages_number                  ##\n",
    "\n",
    "        # Loop through each page and extract the text\n",
    "        for page_num in range(number_of_pages):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            page_text = page.extract_text()\n",
    "            extracted_text+=page_text\n",
    "       \n",
    "    return extracted_text\n",
    "\n",
    "def find_start_and_end(extracted_text): \n",
    "    words_to_find_start = [\"table of contents\", \"contents\", \"index\"]\n",
    "    words_to_find_end = [\"chapter 1\", \"index\", \"preface\", \"Summary\", \"appendix\"]\n",
    "    start = []\n",
    "    end = []\n",
    "    for word in words_to_find_start:\n",
    "        s = extracted_text.lower().find(word)\n",
    "        if s != -1:\n",
    "            start.append(s)\n",
    "    for word in words_to_find_end :\n",
    "        e = extracted_text.lower().find(word)\n",
    "        if e != -1:\n",
    "            end.append(e)\n",
    "    start = sorted(start, reverse=False)\n",
    "    end = sorted(end, reverse=False)\n",
    "    return (start[0],end[0])\n",
    "\n",
    "\n",
    "def roughly_table_of_content(all_extracted_text):\n",
    "    start,end = find_start_and_end(all_extracted_text) \n",
    "\n",
    "    if start !=-1 and end != -1:\n",
    "        modified_text = all_extracted_text[start:end]\n",
    "        return modified_text\n",
    "    else:\n",
    "        return all_extracted_text\n",
    "    \n",
    "## ====================================== Scrapping a book that is images\n",
    "\n",
    "def convert_pdf_to_img(pdf_file):\n",
    "    images = convert_from_path(pdf_file, poppler_path=r\"C:/Users/Mariam Barakat/Desktop/work/CIS_LLM/Git_hub/LLM_Research/Data Preprocessing/poppler/poppler-23.08.0/Library\\bin\", first_page=0, last_page=GOLBAL_pages_number  - 1)\n",
    "    return images\n",
    "\n",
    "def convert_image_to_text(file, place_of_Tesseract = 'C:/Users/Mariam Barakat/AppData/Local/Programs/Tesseract-OCR/tesseract.exe'):\n",
    "    pytesseract.pytesseract.tesseract_cmd = place_of_Tesseract\n",
    "    text = pytesseract.image_to_string(file)\n",
    "    return text\n",
    "\n",
    "def extraction_image_pdf(book_name):\n",
    "    pdf_file = os.path.join( \"..\", \"books\", book_name)\n",
    "    images = convert_pdf_to_img(pdf_file)\n",
    "    final_text = \"\"\n",
    "    for img in images:\n",
    "        out = convert_image_to_text(img)\n",
    "        final_text += out\n",
    "    return final_text\n",
    "## =============================================  Final scrapper\n",
    "\n",
    "def pdf_to_text(book_name):\n",
    "    pdf_path = os.path.join( \"..\", \"books\", book_name)\n",
    "    all_extracted_text1 = extraction_text_pdf(pdf_path)\n",
    "    filtered_text = roughly_table_of_content(all_extracted_text1)\n",
    "\n",
    "    if (all_extracted_text1 == filtered_text):\n",
    "        all_extracted_text2 = extraction_image_pdf(pdf_path)\n",
    "        filtered_text = roughly_table_of_content(all_extracted_text2)\n",
    "    return filtered_text\n",
    "\n",
    "## ================================== Creating a prompt_template for the output \n",
    "def prompt_template(filtered_text):\n",
    "    prompt_template = \"{### Table of contents:\" + filtered_text + \"}\"\n",
    "    # Path to the text file\n",
    "    file_path = \"book_corpus.txt\"\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        # Write the string variable to the file\n",
    "        file.write(prompt_template + '\\n')  # Adding a newline character for clarity\n",
    "\n",
    "## =================================== Loop through the directory to extract all books inside and add to one txt file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_name = \"book1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Big data, machine learning, and more, using Python tools\\n\\nDavy Cielen\\nArno D. B. Meysman\\nMohamed Ali\\n\\ni MANNING\\n\\nIntroducing Data Science\\nIntroducing\\nData Science\\n\\nBIG DATA, MACHINE LEARNING,\\nAND MORE, USING PYTHON TOOLS\\n\\nDAVY CIELEN\\nARNO D. B. MEYSMAN\\nMOHAMED ALI\\n\\nMANNING\\nSHELTER ISLAND\\nFor online information and ordering of this and other Manning books, please visit\\nwww.manning.com. The publisher offers discounts on this book when ordered in quantity.\\nFor more information, please contact\\n\\nSpecial Sales Department\\nManning Publications Co.\\n\\n20 Baldwin Road\\n\\nPO Box 761\\n\\nShelter Island, NY 11964\\nEmail: orders@manning.com\\n\\n©2016 by Manning Publications Co. All rights reserved.\\n\\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in\\nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written\\npermission of the publisher.\\n\\nMany of the designations used by manufacturers and sellers to distinguish their products are\\nclaimed as trademarks. Where those designations appear in the book, and Manning\\nPublications was aware of a trademark claim, the designations have been printed in initial caps\\nor all caps.\\n\\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have\\nthe books we publish printed on acid-free paper, and we exert our best efforts to that end.\\nRecognizing also our responsibility to conserve the resources of our planet, Manning books\\nare printed on paper that is at least 15 percent recycled and processed without the use of\\nelemental chlorine.\\n\\nManning Publications Co.\\n20 Baldwin Road\\n\\nPO Box 761\\n\\nShelter Island, NY 11964\\n\\nTechnical proofreader:\\n\\nTypesetter:\\nCover designer:\\n\\nISBN: 9781633430037\\nPrinted in the United States of America\\n123456789 10 - EBM - 21 20 19 18 17 16\\n\\nDevelopment editor:\\n\\nTechnical development editors:\\nCopyeditor:\\n\\nProofreader:\\n\\nDan Maharry\\n\\nMichael Roberts, Jonathan Thoms\\nKatie Petito\\n\\nAlyson Brener\\n\\nRavishankar Rajagopalan\\n\\nDennis Dalinnik\\n\\nMarija Tudor\\nbrief contents\\n\\n© MN DA KR WH RK\\n\\nData science in a big data world 1\\n\\nThe data science process 22\\n\\nMachine learning 57\\n\\nHandling large data on a single computer 85\\nFirst steps in big data 119\\n\\nJoin the NoSQL movement 150\\n\\nThe rise of graph databases 190\\n\\nText mining and text analytics 218\\n\\nData visualization to the end user 253\\ncontents\\n\\npreface xiii\\n\\nacknowledgments xiv\\n\\nabout this book xvi\\n\\nabout the authors xviii\\n\\nabout the cover illustration xx\\n\\nData science in a big data world 1\\n\\n1.1 Benefits and uses of data science and big data 2\\n1.2 Facetsofdata 4\\n\\nStructured data 4 = Unstructured data 5\\n\\nNatural language 5 * Machine-generated data 6\\n\\nGraph-based or network data 7 = Audio, image, and video 8\\nStreaming data 8\\n\\n1.3. The data science process 8\\n\\nSetting the research goal 8 = Retrieving data 9\\nData preparation 9 = Data exploration 9\\n\\nData modeling or model building 9 = Presentation\\nand automation 9\\n\\n1.4 The big data ecosystem and data science 10\\n\\nDistributed file systems 10 * Distributed programming\\nframework 12 = Data integration framework 12\\nviii CONTENTS\\n\\nMachine learning frameworks 12\" NoSQUL databases 13\\nScheduling tools 14 = Benchmarking tools 14\\nSystem deployment 14 * Service programming 14\\nSecurity 14\\n\\n1.5 An introductory working example of Hadoop 15\\n\\n1.6 Summary 20\\n\\nThe data science process 22\\n\\n2.1 Overview of the data science process 22\\n\\nDon’t be a slave to the process 25\\n\\n2.2 Step 1: Defining research goals and creating\\na project charter 25\\nSpend time understanding the goals and context of your research 26\\nCreate a project charter 26\\n\\n2.3. Step 2: Retrieving data 27\\nStart with data stored within the company 28 * Don’t be afraid\\nto shop around 28 = Do data quality checks now to prevent\\nproblems later 29\\n\\n2.4 Step 3: Cleansing, integrating, and transforming data 29\\nCleansing data 30 * Correct errors as early as possible 36\\nCombining data from different data sources 37\\nTransforming data 40\\n\\n2.5 Step 4: Exploratory data analysis 43\\n\\n2.6 Step 5: Build the models 48\\nModel and variable selection 48 = Model execution 49\\nModel diagnostics and model comparison 54\\n\\n2.7 Step 6: Presenting findings and building applications on\\ntop of them 55\\n\\n2.8 Summary 56\\n\\nMachine learning 57\\n3.1 What is machine learning and why should you care\\nabout it? 58\\n\\nApplications for machine learning in data science 58\\nWhere machine learning is used in the data science process 59\\nPython tools used in machine learning 60\\nCONTENTS\\n\\n3.2. The modeling process 62\\nEngineering features and selecting a model 62 Training\\nyour model 64\" Validating a model 64 *® Predicting\\nnew observations 65\\n3.3 Types of machine learning 65\\nSupervised learning 66 = Unsupervised learning 72\\n3.4 Semi-supervised learning 82\\n\\n3.5 Summary 83\\n\\nHandling large data on a single computer 85\\n4.1 The problems you face when handling large data 86\\n\\n4.2. General techniques for handling large volumes of data 87\\nChoosing the right algorithm 88 * Choosing the right data\\nstructure 96 * Selecting the right tools 99\\n\\n4.3 General programming tips for dealing with\\nlarge data sets 101\\nDon\\'t reinvent the wheel 101 © Get the most out of your\\nhardware 102 = Reduce your computing needs 102\\n\\n4.4 Case study 1: Predicting malicious URLs 103\\n\\nStep 1: Defining the research goal 104 * Step 2: Acquiring\\nthe URL data 104 = Step 4: Data exploration 105\\nStep 5: Model building 106\\n\\n4.5 Case study 2: Building a recommender system inside\\nadatabase 108\\nTools and techniques needed 108 * Step 1: Research\\nquestion 111 = Step 3: Data preparation 111\\n\\nStep 5: Model building 115 = Step 6: Presentation\\nand automation 116\\n\\n4.6 Summary 118\\n\\nFirst steps in big data 119\\n5.1 Distributing data storage and processing with\\nframeworks 120\\n\\nHadoop: a framework for storing and processing large data sets 121\\nSpark: replacing MapReduce for better performance 123\\n\\nix\\nCONTENTS\\n\\n5.2 Case study: Assessing risk when loaning money 125\\nStep 1: The research goal 126 = Step 2: Data retrieval 127\\n\\nStep 3: Data preparation 131 * Step 4: Data exploration &\\nStep 6: Report building 135\\n\\n5.3. Summary 149\\n\\n6 Join the NoSQL movement 150\\n\\n6.1 Introduction to NoSQL_ 153\\n\\nACID: the core principle of relational databases 153\\nCAP Theorem: the problem with DBs on many nodes 154\\nThe BASE principles of NoSQL databases 156\\nNoSQL database types 158\\n6.2 Case study: What disease is that? 164\\nStep 1: Setting the research goal 166 * Steps 2 and 3: Data\\nretrieval and preparation 167 = Step 4: Data exploration 175\\nStep 3 revisited: Data preparation for disease profiling 183\\n\\nStep 4 revisited: Data exploration for disease profiling 187\\nStep 6: Presentation and automation 188\\n\\n6.3. Summary 189\\n\\nThe rise of graph databases 190\\n\\n7.1 Introducing connected data and graph databases 191\\nWhy and when should I use a graph database? 193\\n\\n7.2. Introducing Neo4j: a graph database 196\\nCypher: a graph query language 198\\n\\n7.3. Connected data example: a recipe recommendation\\nengine 204\\nStep 1: Setting the research goal 205 * Step 2: Data retrieval 206\\n\\nStep 3: Data preparation 207 = Step 4: Data exploration 210\\nStep 5: Data modeling 212 = Step 6: Presentation 216\\n\\n7.4 Summary 216\\n\\nText mining and text analytics 218\\n8.1 Text mining in the real world 220\\n\\n8.2 Text mining techniques 225\\n\\nBag of words 225 = Stemming and lemmatization 227\\nDecision tree classifier 228\\n8.3\\n\\n8.4\\n\\nCONTENTS\\n\\nCase study: Classifying Reddit posts 230\\nMeet the Natural Language Toolkit 231 = Data science process\\noverview and step 1: The research goal 233 = Step 2: Data\\nretrieval 234 = Step 3: Data preparation 237 * Step 4:\\nData exploration 240 = Step 3 revisited: Data preparation\\nadapted 242 * Step 5: Data analysis 246 * Step 6:\\nPresentation and automation 250\\n\\nSummary 252\\n\\nData visualization to the end user 253\\n\\n9.1\\n9.2\\n\\n9.3\\n9.4\\n9.5\\n\\nappendix A\\nappendix B\\nappendix C\\nappendix D\\n\\nData visualization options 254\\nCrossfilter, the JavaScript MapReduce library 257\\n\\nSetting up everything 258 = Unleashing Crossfilter to filter the\\nmedicine data set 262\\n\\nCreating an interactive dashboard with dc.js_ 267\\nDashboard development tools 272\\nSummary 273\\n\\nSetting up Elasticsearch 275\\n\\nSetting up Neo4j 281\\n\\nInstalling MySQL server 284\\n\\nSetting up Anaconda with a virtual environment 288\\n\\nindex 291\\npreface\\n\\nIt’s in all of us. Data science is what makes us humans what we are today. No, not the\\ncomputer-driven data science this book will introduce you to, but the ability of our\\nbrains to see connections, draw conclusions from facts, and learn from our past expe-\\nriences. More so than any other species on the planet, we depend on our brains for\\nsurvival; we went all-in on these features to earn our place in nature. That strategy has\\nworked out for us so far, and we’re unlikely to change it in the near future.\\n\\nBut our brains can only take us so far when it comes to raw computing. Our biol-\\nogy can’t keep up with the amounts of data we can capture now and with the extent of\\nour curiosity. So we turn to machines to do part of the work for us: to recognize pat-\\nterns, create connections, and supply us with answers to our numerous questions.\\n\\nThe quest for knowledge is in our genes. Relying on computers to do part of the\\njob for us is not—but it is our destiny.\\n\\nxiii\\nacknowledgments\\n\\nA big thank you to all the people of Manning involved in the process of making this\\nbook for guiding us all the way through.\\n\\nOur thanks also go to Ravishankar Rajagopalan for giving the manuscript a full\\ntechnical proofread, and to Jonathan Thoms and Michael Roberts for their expert\\ncomments. There were many other reviewers who provided invaluable feedback\\nthroughout the process: Alvin Raj, Arthur Zubarev, Bill Martschenko, Craig Smith,\\nFilip Pravica, Hamideh Iraj, Heather Campbell, Hector Cuesta, Ian Stirk, Jeff Smith,\\nJoel Kotarski, Jonathan Sharley, Jorn Dinkla, Marius Butuc, Matt R. Cole, Matthew\\nHeck, Meredith Godar, Rob Agle, Scott Chaussee, and Steve Rogers.\\n\\nFirst and foremost I want to thank my wife Filipa for being my inspiration and motiva-\\ntion to beat all difficulties and for always standing beside me throughout my career\\nand the writing of this book. She has provided me the necessary time to pursue my\\ngoals and ambition, and shouldered all the burdens of taking care of our little daugh-\\nter in my absence. I dedicate this book to her and really appreciate all the sacrifices\\nshe has made in order to build and maintain our little family.\\n\\nI also want to thank my daughter Eva, and my son to be born, who give me a great\\nsense of joy and keep me smiling. They are the best gifts that God ever gave to my life and\\nalso the best children a dad could hope for: fun, loving, and always a joy to be with.\\n\\nA special thank you goes to my parents for their support over the years. Without\\nthe endless love and encouragement from my family, I would not have been able to\\nfinish this book and continue the journey of achieving my goals in life.\\nACKNOWLEDGMENTS xv\\n\\nI\\'d really like to thank all my coworkers in my company, especially Mo and Arno,\\nfor all the adventures we have been through together. Mo and Arno have provided me\\nexcellent support and advice. I appreciate all of their time and effort in making this\\nbook complete. They are great people, and without them, this book may not have\\nbeen written.\\n\\nFinally, a sincere thank you to my friends who support me and understand that I\\ndo not have much time but I still count on the love and support they have given me\\nthroughout my career and the development of this book.\\n\\nDAVY CIELEN\\n\\nI would like to give thanks to my family and friends who have supported me all the way\\nthrough the process of writing this book. It has not always been easy to stay at home\\nwriting, while I could be out discovering new things. I want to give very special thanks\\nto my parents, my brother Jago, and my girlfriend Delphine for always being there for\\nme, regardless of what crazy plans I come up with and execute.\\n\\nI would also like to thank my godmother, and my godfather whose current struggle\\nwith cancer puts everything in life into perspective again.\\n\\nThanks also go to my friends for buying me beer to distract me from my work and\\nto Delphine’s parents, her brother Karel, and his soon-to-be wife Tess for their hospi-\\ntality (and for stuffing me with good food).\\n\\nAll of them have made a great contribution to a wonderful life so far.\\n\\nLast but not least, I would like to thank my coauthor Mo, my ERC-homie, and my\\ncoauthor Davy for their insightful contributions to this book. I share the ups and\\ndowns of being an entrepreneur and data scientist with both of them on a daily basis.\\nIt has been a great trip so far. Let’s hope there are many more days to come.\\n\\nARNO D. B. MEYSMAN\\n\\nFirst and foremost, I would like to thank my fiancée Muhuba for her love, understand-\\ning, caring, and patience. Finally, I owe much to Davy and Arno for having fun and for\\nmaking an entrepreneurial dream come true. Their unfailing dedication has been a\\nvital resource for the realization of this book.\\n\\nMOHAMED ALI\\nabout this book\\n\\nI can only show you the door. You’re the one that has to walk through it.\\n\\nMorpheus, The Matrix\\n\\nWelcome to the book! When reading the table of contents, you probably noticed\\nthe diversity of the topics we’re about to cover. The goal of Introducing Data Science\\nis to provide you with a little bit of everything—enough to get you started. Data sci-\\nence is a very wide field, so wide indeed that a book ten times the size of this one\\nwouldn’t be able to cover it all. For each chapter, we picked a different aspect we\\nfind interesting. Some hard decisions had to be made to keep this book from col-\\nlapsing your bookshelf!\\n\\nWe hope it serves as an entry point—your doorway into the exciting world of\\ndata science.\\n\\nRoadmap\\n\\nChapters 1 and 2 offer the general theoretical background and framework necessary\\nto understand the rest of this book:\\n\\n= Chapter | is an introduction to data science and big data, ending with a practi-\\ncal example of Hadoop.\\n\\n= Chapter 2 is all about the data science process, covering the steps present in\\nalmost every data science project.\\nABOUT THIS BOOK xvii\\n\\nIn chapters 3 through 5, we apply machine learning on increasingly large data sets:\\n\\n= Chapter 3 keeps it small. The data still fits easily into an average computer’s\\nmemory.\\n\\n= Chapter 4 increases the challenge by looking at “large data.” This data fits on\\nyour machine, but fitting it into RAM is hard, making it a challenge to process\\nwithout a computing cluster.\\n\\n= Chapter 5 finally looks at big data. For this we can’t get around working with\\nmultiple computers.\\n\\nChapters 6 through 9 touch on several interesting subjects in data science in a more-\\nor-less independent matter:\\n\\n= Chapter 6 looks at NoSQL and how it differs from the relational databases.\\n\\n= Chapter 7 applies data science to streaming data. Here the main problem is not\\nsize, but rather the speed at which data is generated and old data becomes\\nobsolete.\\n\\n= Chapter 8 is all about text mining. Not all data starts off as numbers. Text min-\\ning and text analytics become important when the data is in textual formats\\nsuch as emails, blogs, websites, and so on.\\n\\n= Chapter 9 focuses on the last part of the data science process—data visualization\\nand prototype application building—by introducing a few useful HTML5 tools.\\n\\nAppendixes A-D cover the installation and setup of the Elasticsearch, Neo4j, and\\nMySQL databases described in the chapters and of Anaconda, a Python code package\\nthat\\'s especially useful for data science.\\n\\nWhom this book is for\\n\\nThis book is an introduction to the field of data science. Seasoned data scientists will\\nsee that we only scratch the surface of some topics. For our other readers, there are\\nsome prerequisites for you to fully enjoy the book. A minimal understanding of SQL,\\nPython, HTML5, and statistics or machine learning is recommended before you dive\\ninto the practical examples.\\n\\nCode conventions and downloads\\n\\nWe opted to use the Python script for the practical examples in this book. Over the\\npast decade, Python has developed into a much respected and widely used data sci-\\nence language.\\n\\nThe code itself is presented in a fixed-width font like this to separate it from\\nordinary text. Code annotations accompany many of the listings, highlighting impor-\\ntant concepts.\\n\\nThe book contains many code examples, most of which are available in the online\\ncode base, which can be found at the book’s website, https://www.manning.com/\\nbooks/introducing-data-science.\\nabout the authors\\n\\nDAVy CIELEN is an experienced entrepreneur, book author, and\\nprofessor. He is the co-owner with Arno and Mo of Optimately\\nand Maiton, two data science companies based in Belgium and\\nthe UK, respectively, and co-owner of a third data science com-\\npany based in Somaliland. The main focus of these companies is\\non strategic big data science, and they are occasionally consulted\\nby many large companies. Davy is an adjunct professor at the\\nIESEG School of Management in Lille, France, where he is\\ninvolved in teaching and research in the field of big data science.\\n\\nARNO MEYSMAN is a driven entrepreneur and data scientist. He is\\nthe co-owner with Davy and Mo of Optimately and Maiton, two\\ndata science companies based in Belgium and the UK, respec-\\ntively, and co-owner of a third data science company based in\\nSomaliland. The main focus of these companies is on strategic\\nbig data science, and they are occasionally consulted by many\\nlarge companies. Arno is a data scientist with a wide spectrum of\\ninterests, ranging from medical analysis to retail to game analytics.\\nHe believes insights from data combined with some imagination\\ncan go a long way toward helping us to improve this world.\\n\\nxviii\\nABOUT THE AUTHORS xix\\n\\nMOHAMED ALI is an entrepreneur and a data science consultant.\\nTogether with Davy and Arno, he is the co-owner of Optimately\\nand Maiton, two data science companies based in Belgium and\\nthe UK, respectively. His passion lies in two areas, data science\\nand sustainable projects, the latter being materialized through\\nthe creation of a third company based in Somaliland.\\n\\nAuthor Online\\n\\nThe purchase of Introducing Data Science includes free access to a private web forum\\nrun by Manning Publications where you can make comments about the book, ask\\ntechnical questions, and receive help from the lead author and from other users. To\\naccess the forum and subscribe to it, point your web browser to https://www.manning\\n.com/books/introducing-data-science. This page provides information on how to get\\non the forum once you are registered, what kind of help is available, and the rules of\\nconduct on the forum.\\n\\nManning’s commitment to our readers is to provide a venue where a meaningful\\ndialog between individual readers and between readers and the author can take place.\\nIt is not a commitment to any specific amount of participation on the part of the\\nauthor, whose contribution to AO remains voluntary (and unpaid). We suggest you try\\nasking the author some challenging questions lest his interest stray! The Author\\nOnline forum and the archives of previous discussions will be accessible from the pub-\\nlisher’s website as long as the book is in print.\\nabout the cover illustration\\n\\nThe illustration on the cover of Introducing Data Science is taken from the 1805 edition\\nof Sylvain Maréchal’s four-volume compendium of regional dress customs. This book\\nwas first published in Paris in 1788, one year before the French Revolution. Each illus-\\ntration is colored by hand. The caption for this illustration reads “Homme Sala-\\nmanque,” which means man from Salamanca, a province in western Spain, on the\\nborder with Portugal. The region is known for its wild beauty, lush forests, ancient oak\\ntrees, rugged mountains, and historic old towns and villages.\\n\\nThe Homme Salamanque is just one of many figures in Maréchal’s colorful collec-\\ntion. Their diversity speaks vividly of the uniqueness and individuality of the world’s\\ntowns and regions just 200 years ago. This was a time when the dress codes of two\\nregions separated by a few dozen miles identified people uniquely as belonging to one\\nor the other. The collection brings to life a sense of the isolation and distance of that\\nperiod and of every other historic period—except our own hyperkinetic present.\\n\\nDress codes have changed since then and the diversity by region, so rich at the\\ntime, has faded away. It is now often hard to tell the inhabitant of one continent from\\nanother. Perhaps we have traded cultural diversity for a more varied personal life—\\ncertainly for a more varied and fast-paced technological life.\\n\\nWe at Manning celebrate the inventiveness, the initiative, and the fun of the com-\\nputer business with book covers based on the rich diversity of regional life two centu-\\nries ago, brought back to life by Maréchal’s pictures.\\nData science in\\na big data world\\n\\nThis chapter covers\\n\\nDefining data science and big data\\nRecognizing the different types of data\\nGaining insight into the data science process\\n\\nIntroducing the fields of data science and\\nbig data\\n\\n= Working through examples of Hadoop\\n\\nBig data is a blanket term for any collection of data sets so large or complex that it\\nbecomes difficult to process them using traditional data management techniques\\nsuch as, for example, the RDBMS (relational database management systems). The\\nwidely adopted RDBMS has long been regarded as a one-size-fits-all solution, but the\\ndemands of handling big data have shown otherwise. Data science involves using\\nmethods to analyze massive amounts of data and extract the knowledge it contains.\\nYou can think of the relationship between big data and data science as being like\\nthe relationship between crude oil and an oil refinery. Data science and big data\\nevolved from statistics and traditional data management but are now considered to\\nbe distinct disciplines.\\n11\\n\\nCuapteR 1 Data science in a big data world\\n\\nThe characteristics of big data are often referred to as the three Vs:\\n\\n= Volume—How much data is there?\\n= Variety—How diverse are different types of data?\\n\\n= Velocity—At what speed is new data generated?\\n\\nOften these characteristics are complemented with a fourth V, veracity: How accu-\\nrate is the data? These four properties make big data different from the data found\\nin traditional data management tools. Consequently, the challenges they bring can\\nbe felt in almost every aspect: data capture, curation, storage, search, sharing, trans-\\nfer, and visualization. In addition, big data calls for specialized techniques to extract\\nthe insights.\\n\\nData science is an evolutionary extension of statistics capable of dealing with the\\nmassive amounts of data produced today. It adds methods from computer science to\\nthe repertoire of statistics. In a research note from Laney and Kart, Emerging Role of\\nthe Data Scientist and the Art of Data Science, the authors sifted through hundreds of\\n\\njob descriptions for data scientist, statistician, and BI (Business Intelligence) analyst\\n\\nto detect the differences between those titles. The main things that set a data scien-\\ntist apart from a statistician are the ability to work with big data and experience in\\nmachine learning, computing, and algorithm building. Their tools tend to differ\\ntoo, with data scientist job descriptions more frequently mentioning the ability to\\nuse Hadoop, Pig, Spark, R, Python, and Java, among others. Don’t worry if you feel\\nintimidated by this list; most of these will be gradually introduced in this book,\\nthough we’ll focus on Python. Python is a great language for data science because it\\nhas many data science libraries available, and it’s widely supported by specialized\\nsoftware. For instance, almost every popular NoSQL database has a Python-specific\\nAPI. Because of these features and the ability to prototype quickly with Python while\\nkeeping acceptable performance, its influence is steadily growing in the data sci-\\nence world.\\n\\nAs the amount of data continues to grow and the need to leverage it becomes\\nmore important, every data scientist will come across big data projects throughout\\ntheir career.\\n\\nBenefits and uses of data science and big data\\n\\nData science and big data are used almost everywhere in both commercial and non-\\ncommercial settings. The number of use cases is vast, and the examples we’ll provide\\nthroughout this book only scratch the surface of the possibilities.\\n\\nCommercial companies in almost every industry use data science and big data to\\ngain insights into their customers, processes, staff, completion, and products. Many\\ncompanies use data science to offer customers a better user experience, as well as to\\ncross-sell, up-sell, and personalize their offerings. A good example of this is Google\\nAdSense, which collects data from internet users so relevant commercial messages can\\nbe matched to the person browsing the internet. MaxPoint (http://maxpoint.com/us)\\nBenefits and uses of data science and big data 3\\n\\nis another example of real-time personalized advertising. Human resource profession-\\nals use people analytics and text mining to screen candidates, monitor the mood of\\nemployees, and study informal networks among coworkers. People analytics is the cen-\\ntral theme in the book Moneyball: The Art of Winning an Unfair Game. In the book (and\\nmovie) we saw that the traditional scouting process for American baseball was ran-\\ndom, and replacing it with correlated signals changed everything. Relying on statistics\\nallowed them to hire the right players and pit them against the opponents where they\\nwould have the biggest advantage. Financial institutions use data science to predict\\nstock markets, determine the risk of lending money, and learn how to attract new cli-\\nents for their services. At the time of writing this book, at least 50% of trades world-\\nwide are performed automatically by machines based on algorithms developed by\\nquants, as data scientists who work on trading algorithms are often called, with the\\nhelp of big data and data science techniques.\\n\\nGovernmental organizations are also aware of data’s value. Many governmental\\norganizations not only rely on internal data scientists to discover valuable informa-\\ntion, but also share their data with the public. You can use this data to gain insights or\\nbuild data-driven applications. Data.gov is but one example; it’s the home of the US\\nGovernment’s open data. A data scientist in a governmental organization gets to work\\non diverse projects such as detecting fraud and other criminal activity or optimizing\\nproject funding. A well-known example was provided by Edward Snowden, who leaked\\ninternal documents of the American National Security Agency and the British Govern-\\nment Communications Headquarters that show clearly how they used data science\\nand big data to monitor millions of individuals. Those organizations collected 5 bil-\\nlion data records from widespread applications such as Google Maps, Angry Birds,\\nemail, and text messages, among many other data sources. Then they applied data sci-\\nence techniques to distill information.\\n\\nNongovernmental organizations (NGOs) are also no strangers to using data. They\\nuse it to raise money and defend their causes. The World Wildlife Fund (WWF), for\\ninstance, employs data scientists to increase the effectiveness of their fundraising\\nefforts. Many data scientists devote part of their time to helping NGOs, because NGOs\\noften lack the resources to collect data and employ data scientists. DataKind is one\\nsuch data scientist group that devotes its time to the benefit of mankind.\\n\\nUniversities use data science in their research but also to enhance the study experi-\\nence of their students. The rise of massive open online courses (MOOC) produces a\\nlot of data, which allows universities to study how this type of learning can comple-\\nment traditional classes. MOOCs are an invaluable asset if you want to become a data\\nscientist and big data professional, so definitely look at a few of the better-known ones:\\nCoursera, Udacity, and edX. The big data and data science landscape changes quickly,\\nand MOOCs allow you to stay up to date by following courses from top universities. If\\nyou aren’t acquainted with them yet, take time to do so now; you\\'ll come to love them\\nas we have.\\n4 CuapteR 1 Data science in a big data world\\n\\n1.2 Facets of data\\n\\nIn data science and big data you\\'ll come across many different types of data, and each\\nof them tends to require different tools and techniques. The main categories of data\\nare these:\\n\\n= Structured\\n\\n= Unstructured\\n\\n= Natural language\\n\\n= Machine-generated\\n\\n= Graph-based\\n\\n= Audio, video, and images\\n\\n= Streaming\\n\\nLet’s explore all these interesting data types.\\n\\n1.2.1 Structured data\\n\\nStructured data is data that depends on a data model and resides in a fixed field\\nwithin a record. As such, it’s often easy to store structured data in tables within data-\\nbases or Excel files (figure 1.1). SQL, or Structured Query Language, is the preferred\\nway to manage and query data that resides in databases. You may also come across\\nstructured data that might give you a hard time storing it in a traditional relational\\ndatabase. Hierarchical data such as a family tree is one such example.\\n\\nThe world isn’t made up of structured data, though; it’s imposed upon it by\\nhumans and machines. More often, data comes unstructured.\\n\\n1 Indicator ID [Dimension List Timeframe Numeric Value Missing Value Flag Confidence Inte\\n2 214390830 |rotal (Age-adjusted) \"2008 74.6% 73.8%\\n3 |214390833 |aged 18-44 years \"2008 59.4% 58.0%\\n4 |214390831 |aged 18-24 years \"2008 37.4% 34.6%\\n5 |214390832 |aged 25-44 years \"2008 66.9% 65.5%\\n6 |214390836 |aged 45-64 years \"2008 88.6% 87.7%\\n7 |214390834 |Aged 45-54 years \"2008 86.3% 85.1%\\n8 |214390835 [Aged 55-64 years \"2008 91.5% 90.4%\\n9 [214390840 |Aged 65 years and over \"2008 94.6% 93.8%\\n\\n214390837 |Aged 65-74 years 2008 93.6% 92.4%\\n214390838 |Aged 75-84 years 2008 95.6% 94.4%\\n214390839 |Aged 85 years and over 2008 96.0% 94.0%\\n214390841 |Male (Age-adjusted) 2008 72.2% 71.1%\\n214390842 |Female (Age-adjusted) 2008 76.8% 75.9%\\n214390843 |White only (Age-adjusted) 2008 73.8% 72.9%\\n214390844 |Black or African American only (Age-adjusted) 2008 77.0% 75.0%\\n214390845 |American Indian or Alaska Native only (Age-adjusted) 2008 66.5% 57.1%\\n214390846 _|Asian only (Age-adjusted) 2008 80.5% 71.7%\\n214390847 |Native Hawaiian or Other Pacific Islander only (Age-adjusted) 2008 Dsu\\n\\n}214390848 12 or more races (Age-adiusted) 2008 75.6% 69.6%\\n\\nFigure 1.1 An Excel table is an example of structured data.\\n1.2.2\\n\\n12.3\\n\\nFacets of data 5\\n\\n@ New team of UI engineers\\n\\n© CDA@engineer.com\\n\\nToday 10:21\\nTo xyz@program.com y *\\n\\nAn investment banking client of mine has had the go ahead to build a new team of UI engineers to work on\\nvarious areas of a cutting-edge single-dealer trading platform.\\n\\nThey will be recruiting at all levels and paying between 40k & 85k (+ all the usual benefits of the banking\\nworld). | understand you may not be looking. | also understand you may be a contractor. Of the last 3 hires\\nthey brought into the team, two were contractors of 10 years who | honestly thought would never turn to\\nwhat they considered “the dark side.”\\n\\nThis is a genuine opportunity to work in an environment that’s built up for best in industry and allows you to\\ngain commercial experience with all the latest tools, tech, and processes.\\n\\nThere is more information below. | appreciate the spec is rather loose — They are not looking for specialists\\nin Angular / Node / Backbone or any of the other buzz words in particular, rather an “engineer” who can\\nwear many hats and is in touch with current tech & tinkers in their own time.\\n\\nFor more information and a confidential chat, please drop me a reply email. Appreciate you may not have\\nan updated CV, but if you do that would be handy to have a look through if you don’t mind sending.\\n\\nFigure 1.2 Email is simultaneously an example of unstructured data and natural language data.\\n\\nUnstructured data\\n\\nUnstructured data is data that isn’t easy to fit into a data model because the content is\\ncontext-specific or varying. One example of unstructured data is your regular email\\n(figure 1.2). Although email contains structured elements such as the sender, title,\\nand body text, it’s a challenge to find the number of people who have written an\\nemail complaint about a specific employee because so many ways exist to refer to a\\nperson, for example. The thousands of different languages and dialects out there fur-\\nther complicate this.\\n\\nA human-written email, as shown in figure 1.2, is also a perfect example of natural\\nlanguage data.\\n\\nNatural language\\n\\nNatural language is a special type of unstructured data; it’s challenging to process\\nbecause it requires knowledge of specific data science techniques and linguistics.\\n1.2.4\\n\\nCuapteR 1 Data science in a big data world\\n\\nThe natural language processing community has had success in entity recognition,\\ntopic recognition, summarization, text completion, and sentiment analysis, but mod-\\nels trained in one domain don’t generalize well to other domains. Even state-of-the-art\\ntechniques aren’t able to decipher the meaning of every piece of text. This shouldn’t\\nbe a surprise though: humans struggle with natural language as well. It’s ambiguous\\nby nature. The concept of meaning itself is questionable here. Have two people listen\\nto the same conversation. Will they get the same meaning? The meaning of the same\\nwords can vary when coming from someone upset or joyous.\\n\\nMachine-generated data\\n\\nMachine-generated data is information that’s automatically created by a computer,\\nprocess, application, or other machine without human intervention. Machine-generated\\ndata is becoming a major data resource and will continue to do so. Wikibon has fore-\\ncast that the market value of the industrial Internet (a term coined by Frost & Sullivan\\nto refer to the integration of complex physical machinery with networked sensors and\\nsoftware) will be approximately $540 billion in 2020. IDC (International Data Corpo-\\nration) has estimated there will be 26 times more connected things than people in\\n2020. This network is commonly referred to as the internet of things.\\n\\nThe analysis of machine data relies on highly scalable tools, due to its high volume\\nand speed. Examples of machine data are web server logs, call detail records, network\\nevent logs, and telemetry (figure 1.3).\\n\\nCSIPERF: TXCOMMIT; 313236\\n2014-11-28 11:36:13, Info cst 00000153 Creating NT transaction (seq\\n69), objectname [6]\"(null)\"\\n\\n2014-11-28 11:36:13, Info csI 00000154 Created NT transaction (seq 69)\\nresult 0x00000000, handle @0x4eS4\\n\\n2014-11-28 11:36:13, Info cst 00000155@2014/11/28:10:36:13.471\\nBeginning NT transaction commit...\\n\\n2014-11-28 11:36:13, Info csi 00000156@2014/11/28:10:36:13.705 CSI perf\\ntrace:\\n\\nCSIPERF : TXCOMMIT; 273983\\n\\n2014-11-28 11:36:13, Info csi 00000157 Creating NT transaction (seq\\n70), objectname (6]\"(null)\"\\n\\n2014-11-28 11:36:13, Info csi 00000158 Created NI transaction (seq 70)\\nresult 0x00000000, handle @0x4eSc\\n\\n2014-11-28 11:36:13, Info csi 000001S9@2014/11/28:10:36:13.764\\nBeginning NT transaction commit...\\n\\n2014-11-28 11:36:14, Info csi 0000015a@2014/11/28:10:36:14.094 CSI pert\\ntrace:\\n\\nCSIPERF: TXCOMMIT; 386259\\n\\n2014-11-28 11:36:14, Info csi 000001Sb Creating NT transaction (seq\\n71), objectname (6]\"(null)\"\\n\\n2014-11-28 11:36:14, Info csi 000001Sc Created NT transaction (seq 71)\\nresult 0x00000000, handle @0x4eSc\\n\\n2014-11-28 11:36:14, Info csi 0000015d@2014/11/28:10:36:14.106\\nBeginning NT transaction commit...\\n\\n2014-11-28 11:36:14, Info csi 000001Se@2014/11/28:10:36:14.428 CSI perf\\ntrace:\\n\\nCSIPERF: TXCOMMIT; 375581\\n\\nFigure 1.3 Example of machine-generated data\\n1.2.5\\n\\nFacets of data 7\\n\\nThe machine data shown in figure 1.3 would fit nicely in a classic table-structured\\ndatabase. This isn’t the best approach for highly interconnected or “networked” data,\\nwhere the relationships between entities have a valuable role to play.\\n\\nGraph-based or network data\\n\\n“Graph data” can be a confusing term because any data can be shown in a graph.\\n“Graph” in this case points to mathematical graph theory. In graph theory, a graph is a\\nmathematical structure to model pair-wise relationships between objects. Graph or\\nnetwork data is, in short, data that focuses on the relationship or adjacency of objects.\\nThe graph structures use nodes, edges, and properties to represent and store graphi-\\ncal data. Graph-based data is a natural way to represent social networks, and its struc-\\nture allows you to calculate specific metrics such as the influence of a person and the\\nshortest path between two people.\\n\\nExamples of graph-based data can be found on many social media websites (fig-\\nure 1.4). For instance, on LinkedIn you can see who you know at which company.\\nYour follower list on Twitter is another example of graph-based data. The power and\\nsophistication comes from multiple, overlapping graphs of the same nodes. For exam-\\nple, imagine the connecting edges here to show “friends” on Facebook. Imagine\\nanother graph with the same people which connects business colleagues via LinkedIn.\\nImagine a third graph based on movie interests on Netflix. Overlapping the three\\ndifferent-looking graphs makes more interesting questions possible.\\n\\nLucy Guy\\n\\nick Barack\\nEr S “ John\\n\\nFigure 1.4 Friends in a social network are an example of graph-based data.\\n\\nJa\\n\\nGraph databases are used to store graph-based data and are queried with specialized\\nquery languages such as SPARQL.\\n\\nGraph data poses its challenges, but for a computer interpreting additive and\\nimage data, it can be even more difficult.\\n1.2.6\\n\\n1.2.7\\n\\n13\\n\\n13.1\\n\\nCuapteR 1 Data science in a big data world\\n\\nAudio, image, and video\\n\\nAudio, image, and video are data types that pose specific challenges to a data scientist.\\nTasks that are trivial for humans, such as recognizing objects in pictures, turn out to\\nbe challenging for computers. MLBAM (Major League Baseball Advanced Media)\\nannounced in 2014 that they’ll increase video capture to approximately 7 TB per\\ngame for the purpose of live, in-game analytics. High-speed cameras at stadiums will\\ncapture ball and athlete movements to calculate in real time, for example, the path\\ntaken by a defender relative to two baselines.\\n\\nRecently a company called DeepMind succeeded at creating an algorithm that’s\\ncapable of learning how to play video games. This algorithm takes the video screen as\\ninput and learns to interpret everything via a complex process of deep learning. It’s a\\nremarkable feat that prompted Google to buy the company for their own Artificial\\nIntelligence (AI) development plans. The learning algorithm takes in data as it’s pro-\\nduced by the computer game; it’s streaming data.\\n\\nStreaming data\\n\\nWhile streaming data can take almost any of the previous forms, it has an extra\\nproperty. The data flows into the system when an event happens instead of being\\nloaded into a data store in a batch. Although this isn’t really a different type of data,\\nwe treat it here as such because you need to adapt your process to deal with this type\\nof information.\\n\\nExamples are the “What’s trending” on Twitter, live sporting or music events, and\\nthe stock market.\\n\\nThe data science process\\n\\nThe data science process typically consists\\nof six steps, as you can see in the mind map\\n\\nin figure 1.5. We will introduce them briefly\\nhere and handle them in more detail in Data science process\\nchapter 2.\\n1: Setting the research goal |®\\n2: Retrieving data |®\\nData science is mostly applied in the con-\\ntext of an organization. When the business CBDELFISEETEMED |i)\\nasks you to perform a data science project, (4: Data exploration }®\\nyou\\'ll first prepare a project charter. This\\ncharter contains information such as what PREETI) |\\nyou\\'re going to research, how the company 6: Presentation and automation |®\\n\\nbenefits from that, what data and resources\\n\\nSetting the research goal\\n\\nyou need, a timetable, and deliverables. Figure 1.5 The data science process\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_image_pdf(book_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contents\\n\\n© MN DA KR WH RK\\n\\nData science in a big data world 1\\n\\nThe data science process 22\\n\\nMachine learning 57\\n\\nHandling large data on a single computer 85\\nFirst steps in big data 119\\n\\nJoin the NoSQL movement 150\\n\\nThe rise of graph databases 190\\n\\nText mining and text analytics 218\\n\\nData visualization to the end user 253\\ncontents\\n\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roughly_table_of_content(extraction_image_pdf(book_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contents\n",
      "1■Data science in a big data world 1\n",
      "2■The data science process 22\n",
      "3■Machine learning 57\n",
      "4■Handling large data on a single computer 85\n",
      "5■First steps in big data 119\n",
      "6■Join the NoSQL movement 150\n",
      "7■The rise of graph databases 190\n",
      "8■Text mining and text analytics 218\n",
      "9■Data visualization to the end user 253viicontents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf_to_text(book_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= pdf_to_text(book_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
