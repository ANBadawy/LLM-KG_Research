{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Falcon 7B finetuning\nwith the help of this: https://medium.com/@iamarunbrahma/fine-tuning-of-falcon-7b-large-language-model-using-qlora-on-mental-health-dataset-aa290eb6ec85","metadata":{}},{"cell_type":"markdown","source":"## Installation","metadata":{}},{"cell_type":"code","source":"!pip install trl transformers accelerate git+https://github.com/huggingface/peft.git -Uqqq\n!pip install datasets bitsandbytes einops wandb -Uqqq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-21T18:29:38.655067Z","iopub.execute_input":"2023-09-21T18:29:38.655442Z","iopub.status.idle":"2023-09-21T18:30:19.158185Z","shell.execute_reply.started":"2023-09-21T18:29:38.655409Z","shell.execute_reply":"2023-09-21T18:30:19.156609Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\nfrom peft import LoraConfig, get_peft_model, PeftConfig, PeftModel, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n     ","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:58:34.920518Z","iopub.execute_input":"2023-09-21T18:58:34.920953Z","iopub.status.idle":"2023-09-21T18:58:34.928973Z","shell.execute_reply.started":"2023-09-21T18:58:34.920919Z","shell.execute_reply":"2023-09-21T18:58:34.927607Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\nimport json\nimport os\nimport bitsandbytes as bnb\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nimport pandas  as pd\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:32:22.206007Z","iopub.execute_input":"2023-09-21T18:32:22.206487Z","iopub.status.idle":"2023-09-21T18:32:27.449566Z","shell.execute_reply.started":"2023-09-21T18:32:22.206446Z","shell.execute_reply":"2023-09-21T18:32:27.448591Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## loading the dataset","metadata":{}},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/book-corpus2/dataset.csv',encoding = \"utf-8\")\ndf.to_csv(\"data2.csv\",encoding = \"utf-8\", index=False)\n\ntraining_data = load_dataset('csv', data_files='/kaggle/working/data2.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:33:16.848880Z","iopub.execute_input":"2023-09-21T18:33:16.849468Z","iopub.status.idle":"2023-09-21T18:33:17.753111Z","shell.execute_reply.started":"2023-09-21T18:33:16.849436Z","shell.execute_reply":"2023-09-21T18:33:17.752163Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8850778f8cad4f068ee212f57e5b2a76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a704dabf0fcd487385520d58a522f356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1749bff01454e2eb1db5b79b2bc251b"}},"metadata":{}}]},{"cell_type":"code","source":"training_data['train']","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:33:22.172485Z","iopub.execute_input":"2023-09-21T18:33:22.172896Z","iopub.status.idle":"2023-09-21T18:33:22.181007Z","shell.execute_reply.started":"2023-09-21T18:33:22.172864Z","shell.execute_reply":"2023-09-21T18:33:22.179827Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['train'],\n    num_rows: 8\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preparation for Training","metadata":{}},{"cell_type":"code","source":"\nmodel_name = \"ybelkada/falcon-7b-sharded-bf16\" # sharded falcon-7b model\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,            # load model in 4-bit precision\n    bnb_4bit_quant_type=\"nf4\",    # pre-trained model should be quantized in 4-bit NF format\n    bnb_4bit_use_double_quant=True, # Using double quantization as mentioned in QLoRA paper\n    bnb_4bit_compute_dtype=torch.bfloat16, # During computation, pre-trained model should be loaded in BF16 format\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config, # Use bitsandbytes config\n    device_map=\"auto\",  # Specifying device_map=\"auto\" so that HF Accelerate will determine which GPU to put each layer of the model on\n    trust_remote_code=True, # Set trust_remote_code=True to use falcon-7b model with custom code\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:33:26.805894Z","iopub.execute_input":"2023-09-21T18:33:26.806262Z","iopub.status.idle":"2023-09-21T18:35:16.980393Z","shell.execute_reply.started":"2023-09-21T18:33:26.806232Z","shell.execute_reply":"2023-09-21T18:35:16.979458Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f2a5b02c485432bbe5a16580a68b95f"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token # Setting pad_token same as eos_token","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:35:29.590798Z","iopub.execute_input":"2023-09-21T18:35:29.591514Z","iopub.status.idle":"2023-09-21T18:35:30.325709Z","shell.execute_reply.started":"2023-09-21T18:35:29.591467Z","shell.execute_reply":"2023-09-21T18:35:30.324699Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Configuration settings for PEFT model and get PEFT model:","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\n\nlora_alpha = 16 # scaling factor for the weight matrices\nlora_dropout = 0.1 # dropout probability of the LoRA layers\nlora_rank = 8 # dimension of the low-rank matrices\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_rank,\n    bias=\"none\",  # setting to 'none' for only training weight params instead of biases\n    task_type=\"CAUSAL_LM\",\n    target_modules=[         # Setting names of modules in falcon-7b model that we want to apply LoRA to\n        \"query_key_value\",\n        \"dense\",\n        \"dense_h_to_4h\",\n        \"dense_4h_to_h\",\n    ]\n)\n\npeft_model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:37:45.040674Z","iopub.execute_input":"2023-09-21T18:37:45.041414Z","iopub.status.idle":"2023-09-21T18:38:43.457356Z","shell.execute_reply.started":"2023-09-21T18:37:45.041378Z","shell.execute_reply":"2023-09-21T18:38:43.456267Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Configuration Settings for TrainingArguments and Trainer:","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:39:16.938465Z","iopub.execute_input":"2023-09-21T18:39:16.938889Z","iopub.status.idle":"2023-09-21T18:39:16.967154Z","shell.execute_reply.started":"2023-09-21T18:39:16.938853Z","shell.execute_reply":"2023-09-21T18:39:16.966238Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76068af69ac548b9b37cc75416958f0b"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTTrainer\noutput_dir = \"/kaggle/working/falcon-7b-sharded-bf16-finetuned\"\nper_device_train_batch_size = 2 # reduce batch size by 2x if out-of-memory error\ngradient_accumulation_steps = 32  # increase gradient accumulation steps by 2x if batch size is reduced\noptim = \"paged_adamw_32bit\" # activates the paging for better memory management\nsave_strategy=\"steps\" # checkpoint save strategy to adopt during training\nsave_steps = 10 # number of updates steps before two checkpoint saves\nlogging_steps = 10  # number of update steps between two logs if logging_strategy=\"steps\"\nlearning_rate = 2e-4  # learning rate for AdamW optimizer\nmax_grad_norm = 0.3 # maximum gradient norm (for gradient clipping)\nmax_steps = 30       # training will happen for 30 steps\nwarmup_ratio = 0.03 # number of steps used for a linear warmup from 0 to learning_rate\nlr_scheduler_type = \"cosine\"  # learning rate scheduler\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    push_to_hub=True,\n)\n\ntrainer = SFTTrainer(\n    model=peft_model,\n    train_dataset=training_data['train'],\n    peft_config=peft_config,\n    dataset_text_field=\"train\",\n    max_seq_length=100,\n    tokenizer=tokenizer,\n    args=training_arguments,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:39:43.366709Z","iopub.execute_input":"2023-09-21T18:39:43.367871Z","iopub.status.idle":"2023-09-21T18:39:44.311357Z","shell.execute_reply.started":"2023-09-21T18:39:43.367826Z","shell.execute_reply":"2023-09-21T18:39:44.310394Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0e3f40a7bb4b779c0cb897e9ac1bac"}},"metadata":{}}]},{"cell_type":"code","source":"# upcasting the layer norms in torch.bfloat16 for more stable training\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.bfloat16)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## training","metadata":{}},{"cell_type":"code","source":"peft_model.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:39:52.873729Z","iopub.execute_input":"2023-09-21T18:39:52.874139Z","iopub.status.idle":"2023-09-21T18:45:08.742152Z","shell.execute_reply.started":"2023-09-21T18:39:52.874108Z","shell.execute_reply":"2023-09-21T18:45:08.741127Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmariambrakat7\u001b[0m (\u001b[33mllm_research_team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.10"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230921_183956-vnwx29uc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/llm_research_team/huggingface/runs/vnwx29uc' target=\"_blank\">rich-rain-9</a></strong> to <a href='https://wandb.ai/llm_research_team/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/llm_research_team/huggingface' target=\"_blank\">https://wandb.ai/llm_research_team/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/llm_research_team/huggingface/runs/vnwx29uc' target=\"_blank\">https://wandb.ai/llm_research_team/huggingface/runs/vnwx29uc</a>"},"metadata":{}},{"name":"stderr","text":"You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 04:20, Epoch 30/30]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.179900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.034200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.004400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=30, training_loss=0.072816697259744, metrics={'train_runtime': 305.9153, 'train_samples_per_second': 6.276, 'train_steps_per_second': 0.098, 'total_flos': 479461570560000.0, 'train_loss': 0.072816697259744, 'epoch': 30.0})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference pipeline for PEFT model:","metadata":{}},{"cell_type":"code","source":"\n# Loading original model\nmodel_name = \"ybelkada/falcon-7b-sharded-bf16\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Loading PEFT model\nPEFT_MODEL = \"/kaggle/working/falcon-7b-sharded-bf16-finetuned\"\n\nconfig = PeftConfig.from_pretrained(PEFT_MODEL)\npeft_base_model = AutoModelForCausalLM.from_pretrained(\n    config.base_model_name_or_path,\n    return_dict=True,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\npeft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n\npeft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\npeft_tokenizer.pad_token = peft_tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:04:06.212646Z","iopub.execute_input":"2023-09-21T19:04:06.213412Z","iopub.status.idle":"2023-09-21T19:06:55.795985Z","shell.execute_reply.started":"2023-09-21T19:04:06.213375Z","shell.execute_reply":"2023-09-21T19:06:55.794819Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f798b74df654d5eb8be75849b1b5139"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GenerationConfig\ndef generate_original_answer(query):\n    system_prompt = \"\"\"Answer the following question truthfully.\n    If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n    If the question is too complex, respond 'Kindly, consult a professor.'.\"\"\"\n\n    user_prompt = f\"\"\"<HUMAN>: {query}\n    <ASSISTANT>: \"\"\"\n\n    final_prompt = system_prompt + \"\\n\" + user_prompt\n\n    device = \"cuda:0\"\n    dashline = \"-\".join(\"\" for i in range(50))\n\n    encoding = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n    outputs = model.generate(input_ids=encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=100, pad_token_id=tokenizer.eos_token_id, \\\n                                                                                             eos_token_id=tokenizer.eos_token_id, attention_mask=encoding.attention_mask, \\\n                                                                                             temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n    text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    print(dashline)\n    print(f'ORIGINAL MODEL RESPONSE:\\n{text_output}')\n    print(dashline)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:49:27.407706Z","iopub.execute_input":"2023-09-21T18:49:27.408668Z","iopub.status.idle":"2023-09-21T18:49:27.423102Z","shell.execute_reply.started":"2023-09-21T18:49:27.408631Z","shell.execute_reply":"2023-09-21T18:49:27.422017Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\ndef generate_peft_answer(query):\n    system_prompt = \"\"\"Answer the following question truthfully.\n    If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n    If the question is too complex, respond 'Kindly, consult a professor.'.\"\"\"\n\n    user_prompt = f\"\"\"<HUMAN>: {query}\n    <ASSISTANT>: \"\"\"\n\n    final_prompt = system_prompt + \"\\n\" + user_prompt\n\n    device = \"cuda:0\"\n    dashline = \"-\".join(\"\" for i in range(50))\n\n    peft_encoding = peft_tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n    peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = peft_tokenizer.eos_token_id, \\\n                                                                                                                     eos_token_id = peft_tokenizer.eos_token_id, attention_mask = peft_encoding.attention_mask, \\\n                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n    peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n\n    print(f'PEFT MODEL RESPONSE:\\n{peft_text_output}')\n    print(dashline)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:12:42.787230Z","iopub.execute_input":"2023-09-21T19:12:42.787839Z","iopub.status.idle":"2023-09-21T19:12:42.806407Z","shell.execute_reply.started":"2023-09-21T19:12:42.787799Z","shell.execute_reply":"2023-09-21T19:12:42.803865Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import time\n\n# Call the function and measure inference time\nstart_time = time.time()\ngenerate_original_answer(\"generate a course outline for a deep learning course\")\nend_time = time.time()\n\ninference_time = end_time - start_time\nprint(f\"Inference Time: {inference_time} seconds\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-21T18:46:51.931493Z","iopub.execute_input":"2023-09-21T18:46:51.931907Z","iopub.status.idle":"2023-09-21T18:47:57.951472Z","shell.execute_reply.started":"2023-09-21T18:46:51.931872Z","shell.execute_reply":"2023-09-21T18:47:57.950382Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"-------------------------------------------------\nORIGINAL MODEL RESPONSE:\nAnswer the following question truthfully.\n    If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n    If the question is too complex, respond 'Kindly, consult a professor.'.\n<HUMAN>: generate a course outline for a deep learning course\n    <ASSISTANT>: (optional) generate a course outline for a machine learning course\nTable of Contents\nIntroduction\nWhat Will I Learn\nHow Will I Learn It\nWho Is This Book For\nCourse Outline\nPart One: Data Understanding\nData Types and Attributes\nNumeric Attributes\nBinary Attributes\nMultivariate Attributes\nTemporal Attributes\nTypes of Data Mining/Learning\nRegression\nClassification\nFeature Engineering\nData Cleaning\nData Augmentation\nPart Two: Data Preparation\nData Preprocessing\n\n-------------------------------------------------\nInference Time: 66.00922060012817 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"# Call the function and measure inference time\nstart_time = time.time()\ngenerate_peft_answer(\"generate a course outline for a deep learning course\")\nend_time = time.time()\n\ninference_time = end_time - start_time\nprint(f\"Inference Time: {inference_time} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T19:28:15.888898Z","iopub.execute_input":"2023-09-21T19:28:15.889303Z","iopub.status.idle":"2023-09-21T19:32:11.054064Z","shell.execute_reply.started":"2023-09-21T19:28:15.889270Z","shell.execute_reply":"2023-09-21T19:32:11.052939Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"PEFT MODEL RESPONSE:\nAnswer the following question truthfully.\n    If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n    If the question is too complex, respond 'Kindly, consult a professor.'.\n<HUMAN>: generate a course outline for a deep learning course\n    <ASSISTANT>: (Human Resources) please review the course outline and provide feedback\nTable of Contents\nIntroduction\nWhat Makes Deep Learning So Powerful\nHow Does It Work\nTypes of Data that are Suitable for Deep Learning\nThe Feature Engineering Problem\nTruly Understanding Your Data\nA Gentle Start: A Brief History of Machine Learning\nAn In-Depth Look: The Statistical Framework\nUnderstanding Logistic Regression\nUnderstanding Multivariate Gaussians\nUnderstanding Matrix Algebra\nPractical Guide\nWhich Model Should I Use\nChoosing Hyperparameters\nTraining on Validation Data\nMonitoring Training Progress\nCommon Problems and Solutions\nFurther Reading\nAppendix\nMatrix Multiplication Is Composition of Functions\nPython/PyTorch/CPP API Reference\n# 1  What Makes Deep Learning So Powerful\nDeep learning has become one of the most popular machine learning techniques in recent years. Why? Because it works so well! Deep learning models can learn from very little data, generalize to unseen data, and solve problems that were previously considered impossible.\nIn this book, we will explore the inner workings of deep learning. We will start with an introduction to the statistical framework that underlies deep learning. Then, we will move on to feature engineering and understanding your data. Next, we will delve into the\n-------------------------------------------------\nInference Time: 235.1547408103943 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}